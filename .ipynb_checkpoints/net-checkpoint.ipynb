{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>EmpathNet: the New CherryStems algorithm</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the new Empath musical audio generation with deep learning models. The baseline model is using the reference of an amazing blog post by https://www.analyticsvidhya.com/blog/2020/01/how-to-perform-automatic-music-generation/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Step 1: Import libraries</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#library for understanding music\n",
    "from music21 import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Step 2: Read Musical Files</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function for reading the MIDI files, it returns the array of notes and choids present in the musical file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining function to read MIDI files\n",
    "def read_midi(file):\n",
    "    \n",
    "    print(\"Loading Music File:\",file)\n",
    "    \n",
    "    notes=[]\n",
    "    notes_to_parse = None\n",
    "    \n",
    "    #parsing a midi file\n",
    "    midi = converter.parse(file)\n",
    "  \n",
    "    #grouping based on different instruments\n",
    "    s2 = instrument.partitionByInstrument(midi)\n",
    "\n",
    "    #Looping over all the instruments\n",
    "    for part in s2.parts:\n",
    "    \n",
    "        #select elements of only piano\n",
    "        if 'Piano' in str(part): \n",
    "        \n",
    "            notes_to_parse = part.recurse() \n",
    "      \n",
    "            #finding whether a particular element is note or a chord\n",
    "            for element in notes_to_parse:\n",
    "                \n",
    "                #note\n",
    "                if isinstance(element, note.Note):\n",
    "                    notes.append(str(element.pitch))\n",
    "                \n",
    "                #chord\n",
    "                elif isinstance(element, chord.Chord):\n",
    "                    notes.append('.'.join(str(n) for n in element.normalOrder))\n",
    "\n",
    "    return np.array(notes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Step 3: Load the MIDI files into our environment</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Music File: schubert/schumm-1.mid\n",
      "Loading Music File: schubert/schumm-2.mid\n",
      "Loading Music File: schubert/schub_d960_4.mid\n",
      "Loading Music File: schubert/schumm-3.mid\n",
      "Loading Music File: schubert/schub_d960_1.mid\n",
      "Loading Music File: schubert/schumm-6.mid\n",
      "Loading Music File: schubert/schumm-4.mid\n",
      "Loading Music File: schubert/schub_d960_2.mid\n",
      "Loading Music File: schubert/schub_d960_3.mid\n",
      "Loading Music File: schubert/schumm-5.mid\n",
      "Loading Music File: schubert/schuim-4.mid\n",
      "Loading Music File: schubert/schuim-1.mid\n",
      "Loading Music File: schubert/schuim-3.mid\n",
      "Loading Music File: schubert/schuim-2.mid\n",
      "Loading Music File: schubert/schubert_D850_4.mid\n",
      "Loading Music File: schubert/schubert_D935_4.mid\n",
      "Loading Music File: schubert/schub_d760_4.mid\n",
      "Loading Music File: schubert/schubert_D850_1.mid\n",
      "Loading Music File: schubert/schubert_D935_1.mid\n",
      "Loading Music File: schubert/schub_d760_1.mid\n",
      "Loading Music File: schubert/schubert_D850_2.mid\n",
      "Loading Music File: schubert/schub_d760_3.mid\n",
      "Loading Music File: schubert/schubert_D935_2.mid\n",
      "Loading Music File: schubert/schubert_D935_3.mid\n",
      "Loading Music File: schubert/schubert_D850_3.mid\n",
      "Loading Music File: schubert/schub_d760_2.mid\n",
      "Loading Music File: schubert/schu_143_2.mid\n",
      "Loading Music File: schubert/schu_143_3.mid\n",
      "Loading Music File: schubert/schu_143_1.mid\n"
     ]
    }
   ],
   "source": [
    "#for listing down the file names\n",
    "import os\n",
    "\n",
    "#Array Processing\n",
    "import numpy as np\n",
    "\n",
    "#specify the path\n",
    "path='schubert/'\n",
    "\n",
    "#read all the filenames\n",
    "files=[i for i in os.listdir(path) if i.endswith(\".mid\")]\n",
    "\n",
    "#reading each midi file\n",
    "notes_array = np.array([read_midi(path+i) for i in files])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Step 4: Understand the data</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "304\n"
     ]
    }
   ],
   "source": [
    "#converting 2D array into 1D array\n",
    "notes_ = [element for note_ in notes_array for element in note_]\n",
    "\n",
    "#No. of unique notes\n",
    "unique_notes = list(set(notes_))\n",
    "print(len(unique_notes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Now see the distribution of the notes</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([187.,  41.,  26.,  11.,   6.,   9.,  12.,   6.,   3.,   3.]),\n",
       " array([1.0000e+00, 1.4790e+02, 2.9480e+02, 4.4170e+02, 5.8860e+02,\n",
       "        7.3550e+02, 8.8240e+02, 1.0293e+03, 1.1762e+03, 1.3231e+03,\n",
       "        1.4700e+03]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 302,
       "width": 319
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#importing library\n",
    "from collections import Counter\n",
    "\n",
    "#computing frequency of each note\n",
    "#the use of the word 'freq' means the occurence frequency of notes not the musical frequency\n",
    "freq = dict(Counter(notes_))\n",
    "\n",
    "#library for visualiation\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#consider only the frequencies\n",
    "no=[count for _,count in freq.items()]\n",
    "\n",
    "#set the figure size\n",
    "plt.figure(figsize=(5,5))\n",
    "\n",
    "#plot\n",
    "plt.hist(no)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above plot, we can infer that most of the notes have a very low frequency. So, let us keep the top frequent notes and ignore the low-frequency ones. Here, I am defining the threshold as 50. Nevertheless, the parameter can be changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167\n"
     ]
    }
   ],
   "source": [
    "frequent_notes = [note_ for note_, count in freq.items() if count>=50]\n",
    "print(len(frequent_notes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Step 5: Prepare the data</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## prepare new musical files which contain only the top frequent notes\n",
    "\n",
    "new_music=[]\n",
    "\n",
    "for notes in notes_array:\n",
    "    temp=[]\n",
    "    for note_ in notes:\n",
    "        if note_ in frequent_notes:\n",
    "            temp.append(note_)            \n",
    "    new_music.append(temp)\n",
    "    \n",
    "new_music = np.array(new_music)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preparing the input and output sequences as mentioned in the article:\n",
    "no_of_timesteps = 32\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "for note_ in new_music:\n",
    "    for i in range(0, len(note_) - no_of_timesteps, 1):\n",
    "        \n",
    "        #preparing input and output sequences\n",
    "        input_ = note_[i:i + no_of_timesteps]\n",
    "        output = note_[i + no_of_timesteps]\n",
    "        \n",
    "        x.append(input_)\n",
    "        y.append(output)\n",
    "        \n",
    "x=np.array(x)\n",
    "y=np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## assign a unique integer to every note:\n",
    "unique_x = list(set(x.ravel()))\n",
    "x_note_to_int = dict((note_, number) for number, note_ in enumerate(unique_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing input sequences\n",
    "x_seq=[]\n",
    "for i in x:\n",
    "    temp=[]\n",
    "    for j in i:\n",
    "        #assigning unique integer to every note\n",
    "        temp.append(x_note_to_int[j])\n",
    "    x_seq.append(temp)\n",
    "    \n",
    "x_seq = np.array(x_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## prepare output sequences\n",
    "unique_y = list(set(y))\n",
    "y_note_to_int = dict((note_, number) for number, note_ in enumerate(unique_y)) \n",
    "y_seq=np.array([y_note_to_int[i] for i in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 80% of the data for training and 20% for testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_tr, x_val, y_tr, y_val = train_test_split(x_seq,y_seq,test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Step 6: Build Model</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>WaveNet model</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 32, 100)           16700     \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 32, 64)            19264     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 16, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 16, 128)           24704     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 16, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 8, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 8, 256)            98560     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 8, 256)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 4, 256)            0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 167)               42919     \n",
      "=================================================================\n",
      "Total params: 267,939\n",
      "Trainable params: 267,939\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.callbacks import *\n",
    "import keras.backend as K\n",
    "\n",
    "K.clear_session()\n",
    "model = Sequential()\n",
    "    \n",
    "#embedding layer\n",
    "model.add(Embedding(len(unique_x), 100, input_length=32, trainable=True)) \n",
    "\n",
    "model.add(Conv1D(64,3, padding='causal', activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(MaxPool1D(2))\n",
    "    \n",
    "model.add(Conv1D(128,3,activation='relu',dilation_rate=2,padding='causal'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(MaxPool1D(2))\n",
    "\n",
    "model.add(Conv1D(256,3,activation='relu',dilation_rate=4,padding='causal'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(MaxPool1D(2))\n",
    "          \n",
    "#model.add(Conv1D(256,5,activation='relu'))    \n",
    "model.add(GlobalMaxPool1D())\n",
    "    \n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(len(unique_y), activation='softmax'))\n",
    "    \n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>LSTM model</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(LSTM(128,return_sequences=True))\n",
    "model2.add(LSTM(128))\n",
    "model2.add(Dense(256))\n",
    "model2.add(Activation('relu'))\n",
    "model2.add(Dense(256))\n",
    "model2.add(Activation('softmax'))\n",
    "model2.compile(loss='sparse_categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Define the callback to save the best model during training</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc=ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', save_best_only=True,verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Step 7: Training</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading best model from previous training if available\n",
    "from keras.models import load_model\n",
    "\n",
    "try:\n",
    "    previous_model = load_model('best_model.h5')\n",
    "    model = previous_model\n",
    "except ImportError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 51530 samples, validate on 12883 samples\n",
      "Epoch 1/3000\n",
      "51530/51530 [==============================] - 14s 281us/step - loss: 1.8108 - val_loss: 2.5823\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.58233, saving model to best_model.h5\n",
      "Epoch 2/3000\n",
      "51530/51530 [==============================] - 14s 275us/step - loss: 1.8200 - val_loss: 2.5583\n",
      "\n",
      "Epoch 00002: val_loss improved from 2.58233 to 2.55830, saving model to best_model.h5\n",
      "Epoch 3/3000\n",
      "51530/51530 [==============================] - 15s 286us/step - loss: 1.8076 - val_loss: 2.5630\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 2.55830\n",
      "Epoch 4/3000\n",
      "51530/51530 [==============================] - 17s 332us/step - loss: 1.7955 - val_loss: 2.5661\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 2.55830\n",
      "Epoch 5/3000\n",
      "51530/51530 [==============================] - 16s 307us/step - loss: 1.8038 - val_loss: 2.5674\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 2.55830\n",
      "Epoch 6/3000\n",
      "51530/51530 [==============================] - 16s 316us/step - loss: 1.7958 - val_loss: 2.5652\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 2.55830\n",
      "Epoch 7/3000\n",
      "51530/51530 [==============================] - 16s 304us/step - loss: 1.8019 - val_loss: 2.5812\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 2.55830\n",
      "Epoch 8/3000\n",
      "51530/51530 [==============================] - 15s 300us/step - loss: 1.7998 - val_loss: 2.5713\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 2.55830\n",
      "Epoch 9/3000\n",
      "51530/51530 [==============================] - 15s 286us/step - loss: 1.8123 - val_loss: 2.5652\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 2.55830\n",
      "Epoch 10/3000\n",
      "51530/51530 [==============================] - 16s 320us/step - loss: 1.8100 - val_loss: 2.5739\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 2.55830\n",
      "Epoch 11/3000\n",
      "51530/51530 [==============================] - 17s 332us/step - loss: 1.8068 - val_loss: 2.5765\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 2.55830\n",
      "Epoch 12/3000\n",
      "51530/51530 [==============================] - 17s 337us/step - loss: 1.8035 - val_loss: 2.5742\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 2.55830\n",
      "Epoch 13/3000\n",
      "51530/51530 [==============================] - 17s 320us/step - loss: 1.8111 - val_loss: 2.5676\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 2.55830\n",
      "Epoch 14/3000\n",
      "51530/51530 [==============================] - 17s 328us/step - loss: 1.7991 - val_loss: 2.5544\n",
      "\n",
      "Epoch 00014: val_loss improved from 2.55830 to 2.55441, saving model to best_model.h5\n",
      "Epoch 15/3000\n",
      "51530/51530 [==============================] - 17s 329us/step - loss: 1.8105 - val_loss: 2.5767\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 2.55441\n",
      "Epoch 16/3000\n",
      "26752/51530 [==============>...............] - ETA: 8s - loss: 1.7800"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-fae17a396dbd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m                                      np.array(y_val)),\n\u001b[1;32m      8\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                     callbacks=[mc])\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#training wavenet model\n",
    "history = model.fit(np.array(x_tr),\n",
    "                    np.array(y_tr),\n",
    "                    batch_size=128,\n",
    "                    epochs=3000, \n",
    "                    validation_data=(np.array(x_val),\n",
    "                                     np.array(y_val)),\n",
    "                    verbose=1, \n",
    "                    callbacks=[mc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Step 8: Prediction</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading best model\n",
    "from keras.models import load_model\n",
    "model = load_model('best_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[70, 37, 70, 70, 70, 70, 70, 70, 125, 70, 70, 70, 10, 10, 18, 76, 87, 125, 87, 10, 10, 10, 125, 125, 10, 125, 125, 125, 76, 76, 85, 76, 11, 76, 33, 85, 85, 85, 85, 85, 85, 85, 85, 85, 18, 18, 18, 18, 18, 18]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "ind = np.random.randint(0,len(x_val)-1)\n",
    "\n",
    "random_music = x_val[ind]\n",
    "\n",
    "# print(random_music)\n",
    "\n",
    "predictions=[]\n",
    "for i in range(50):\n",
    "\n",
    "    random_music = random_music.reshape(1,no_of_timesteps)\n",
    "\n",
    "    prob  = model.predict(random_music)[0]\n",
    "    y_pred= np.argmax(prob,axis=0)\n",
    "    predictions.append(y_pred)\n",
    "\n",
    "    random_music = np.insert(random_music[0],len(random_music[0]),y_pred)\n",
    "    random_music = random_music[1:]\n",
    "    \n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the intergers back into notes\n",
    "x_int_to_note = dict((number, note_) for number, note_ in enumerate(unique_x)) \n",
    "predicted_notes = [x_int_to_note[i] for i in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_midi(prediction_output):\n",
    "   \n",
    "    offset = 0\n",
    "    output_notes = []\n",
    "\n",
    "    # create note and chord objects based on the values generated by the model\n",
    "    for pattern in prediction_output:\n",
    "        \n",
    "        # pattern is a chord\n",
    "        if ('.' in pattern) or pattern.isdigit():\n",
    "            notes_in_chord = pattern.split('.')\n",
    "            notes = []\n",
    "            for current_note in notes_in_chord:\n",
    "                \n",
    "                cn=int(current_note)\n",
    "                new_note = note.Note(cn)\n",
    "                new_note.storedInstrument = instrument.Violin()\n",
    "                notes.append(new_note)\n",
    "                \n",
    "            new_chord = chord.Chord(notes)\n",
    "            new_chord.offset = offset\n",
    "            output_notes.append(new_chord)\n",
    "            \n",
    "        # pattern is a note\n",
    "        else:\n",
    "            \n",
    "            new_note = note.Note(pattern)\n",
    "            new_note.offset = offset\n",
    "            new_note.storedInstrument = instrument.Violin()\n",
    "            output_notes.append(new_note)\n",
    "\n",
    "        # increase offset each iteration so that notes do not stack\n",
    "        offset += 1\n",
    "    midi_stream = stream.Stream(output_notes)\n",
    "    midi_stream.write('midi', fp='music.mid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_to_midi(predicted_notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
